{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn import svm\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'target']\n",
    "\n",
    "df = pd.read_csv('housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "\n",
    "# target is MEDV\n",
    "x = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "mms= MinMaxScaler()\n",
    "x = pd.DataFrame(mms.fit_transform(x), columns=x.columns)\n",
    "\n",
    "\n",
    "xtrain,xtest,ytrain,ytest= train_test_split(x,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train = np.column_stack((x, y))\n",
    "# Append y_test to x_test as a column\n",
    "housing_test = np.column_stack((xtest, ytest))\n",
    "\n",
    "pd.DataFrame(housing_train, columns=column_names).to_csv('housing_train.csv', index=False)\n",
    "pd.DataFrame(housing_test, columns=column_names).to_csv('housing_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "train = pd.read_csv('housing_train.csv')\n",
    "test = pd.read_csv('housing_test.csv')\n",
    "\n",
    "\n",
    "# Specify the columns to include\n",
    "selected_features = ['LSTAT', 'CRIM', 'RM', 'PTRATIO']\n",
    "\n",
    "# Subset the DataFrame to include only the selected features\n",
    "X_train = train[selected_features]\n",
    "X_train = train.drop(columns=['target'])\n",
    "y_train = train['target']  # Assuming the target variable is 'MEDV'\n",
    "\n",
    "# Feature names\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Fit a linear regression model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Get the coefficients and their corresponding feature names\n",
    "coefficients = reg.coef_\n",
    "coeff_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Sort the coefficients by absolute value in descending order\n",
    "coeff_df['Absolute Coefficient'] = np.abs(coeff_df['Coefficient'])\n",
    "coeff_df_sorted = coeff_df.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.bar(coeff_df_sorted['Feature'], coeff_df_sorted['Coefficient'])\n",
    "plt.xlabel('Features', fontsize=14)  # Increase the fontsize to make the x-labels bigger\n",
    "plt.ylabel('Coefficient', fontsize=14)\n",
    "plt.title('Linear Regression Coefficients \\n (Ordered by Absolute Value)', fontsize=16)\n",
    "plt.xticks(rotation=90, fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.show()\n",
    "\n",
    "coeff_df_sorted\n",
    "abs_coeff_sum = coeff_df_sorted['Absolute Coefficient'].sum()\n",
    "abs_coeff_sum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ls = []\n",
    "\n",
    "# count = 0\n",
    "# for i in range(3):\n",
    "#     for j in range(3):\n",
    "#         if (i != j):\n",
    "#             A = i\n",
    "#             B = j\n",
    "#             input = [(\"None\", A, B, \"clustered\", \"uniform\", \"lin_reg\",\"bos_housing\", \"HOUSING_1.csv\", \"columns\")]*20\n",
    "#             input_ls.extend([*input])\n",
    "#             count += 20\n",
    "\n",
    "# print(count)\n",
    "\n",
    "\n",
    "input = [(\"None\", 0, 1, \"clustered\", \"uniform\", \"lin_reg\",\"bos_housing\", \"HOUSING_ALLTRAINLINREG.csv\", \"columns\")]*10\n",
    "input_ls.extend([*input])\n",
    "input = [(\"None\", 1, 0, \"clustered\", \"uniform\", \"lin_reg\",\"bos_housing\", \"HOUSING_ALLTRAINLINREG.csv\", \"columns\")]*10\n",
    "input_ls.extend([*input])\n",
    "input = [(\"None\", 1, 2, \"clustered\", \"uniform\", \"lin_reg\",\"bos_housing\", \"HOUSING_ALLTRAINLINREG.csv\", \"columns\")]*10\n",
    "input_ls.extend([*input])\n",
    "input = [(\"None\", 2, 1, \"clustered\", \"uniform\", \"lin_reg\",\"bos_housing\", \"HOUSING_ALLTRAINLINREG.csv\", \"columns\")]*10\n",
    "input_ls.extend([*input])\n",
    "input = [(\"None\", 0, 1, \"clustered\", \"uniform\", \"random_forest_reg\",\"bos_housing\", \"HOUSING_ALLTRAINRF.csv\", \"columns\")]*10\n",
    "input_ls.extend([*input])\n",
    "input = [(\"None\", 1, 0, \"clustered\", \"uniform\", \"random_forest_reg\",\"bos_housing\", \"HOUSING_ALLTRAINRF.csv\", \"columns\")]*10\n",
    "input_ls.extend([*input])\n",
    "input = [(\"None\", 1, 2, \"clustered\", \"uniform\", \"random_forest_reg\",\"bos_housing\", \"HOUSING_ALLTRAINRF.csv\", \"columns\")]*10\n",
    "input_ls.extend([*input])\n",
    "input = [(\"None\", 2, 1, \"clustered\", \"uniform\", \"random_forest_reg\",\"bos_housing\", \"HOUSING_ALLTRAINRF.csv\", \"columns\")]*10\n",
    "input_ls.extend([*input])\n",
    "\n",
    "\n",
    "\n",
    "parameter_dict = {i: input_ls for i, input_ls in enumerate(input_ls)}\n",
    "\n",
    "# Write the dictionary to a JSON file\n",
    "with open(\"parameters_housing.json\", \"w\") as json_file:\n",
    "    json.dump(parameter_dict, json_file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections.abc import Iterable\n",
    "import itertools\n",
    "from itertools import chain, combinations\n",
    "from array import array\n",
    "import copy\n",
    "import random\n",
    "import scipy.stats as st\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import csv\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "def findShapleyMC(target_data, test_data, A, owners, n):\n",
    "    \n",
    "    \n",
    "    permutations_dict = {}\n",
    "    # A_data = owners[A]\n",
    "\n",
    "    est_shap = 0\n",
    "    for i in range(n):\n",
    "        # draw a permutation, get coalition from permutation\n",
    "        permutation = np.random.permutation(list(owners.keys()))\n",
    "        S = findCoalitionOne(permutation, A)\n",
    "        permString = listToString(S)\n",
    "\n",
    "        P_data = []\n",
    "\n",
    "        # get data from coalition\n",
    "        if permString not in permutations_dict:\n",
    "            P_data = [item for key in S for item in owners.get(key, [])]\n",
    "            permutations_dict.update({permString: P_data})\n",
    "        else:\n",
    "            P_data = permutations_dict[permString]\n",
    "\n",
    "        # print(P_data)\n",
    "\n",
    "        # transfer data ownership between A and B\n",
    "        # owners.update({A: list(np.append(owners[A], P_data))})\n",
    "\n",
    "        # get utility of the data transfer\n",
    "        if (len(P_data)==0):\n",
    "            est_shap += 1/n*(utilityFunc(target_data, target_data, test_data, owners[A], \"ha\")+1)\n",
    "        else:\n",
    "            est_shap += 1/n*(utilityFunc(target_data, target_data, test_data, list(np.append(owners[A], P_data)), \"ha\")-utilityFunc(target_data, target_data, test_data, P_data, \"ha\"))\n",
    "            # print((utilityLR(target_data, test_data, list(np.append(owners[A], P_data)))-utilityLR(target_data, test_data, P_data)))\n",
    "\n",
    "        # owners.update({A: A_data})\n",
    "\n",
    "\n",
    "    return est_shap\n",
    "\n",
    "def utilityKDE(target_data, test_data, data_owner_data):\n",
    "\n",
    "    data_owner_data = [int(x) for x in data_owner_data]\n",
    "\n",
    "    feature = 11\n",
    "    data = target_data.loc[data_owner_data]\n",
    "    data_column = data.iloc[:, feature].values\n",
    "    data_column = [x for x in data_column]\n",
    "    \n",
    "    target_column = target_data.iloc[:,feature].values\n",
    "    target_column = [x for x in target_column]\n",
    "    \n",
    "    test_column = test_data.iloc[:,feature].values\n",
    "\n",
    "    ground_truth_grid, ground_truth_density = kernel_density_estimation(target_column)\n",
    "\n",
    "    sample_estimation_grid, sample_estimation_density = kernel_density_estimation(data_column)\n",
    "\n",
    "    errors = []\n",
    "    for data_item in test_column:\n",
    "        gt_density = np.interp(data_item, ground_truth_grid, ground_truth_density)\n",
    "        errors.append(abs(gt_density-np.interp(data_item, sample_estimation_grid, sample_estimation_density))/gt_density)\n",
    "\n",
    "    return -np.mean(errors)\n",
    "\n",
    "def findCoalitionOne(permutation, o):\n",
    "    coa = []\n",
    "    for data_owner in permutation:\n",
    "        if (data_owner!=o):\n",
    "            coa.append(data_owner)\n",
    "        else:\n",
    "            return sorted(coa)\n",
    "    return sorted(coa)\n",
    "\n",
    "def listToString(perm):\n",
    "    return ' '.join(str(x) for x in perm)\n",
    "\n",
    "def utilityLR(target_data, test_data, data_owner_data):\n",
    "    if (len(data_owner_data)==0):\n",
    "        return 0\n",
    "    data_owner_data = [int(x) for x in data_owner_data]\n",
    "\n",
    "    data = target_data.loc[data_owner_data]\n",
    "    #print(data)\n",
    "\n",
    "    data_features = data.drop(\"target\", axis=1).values\n",
    "\n",
    "    data_labels = data[\"target\"].values\n",
    "    test_data_features = test_data.drop(\"target\", axis=1).values \n",
    "\n",
    "    test_data_labels = test_data[\"target\"].values\n",
    "    \n",
    "\n",
    "    if (len(set(data_labels))==1):\n",
    "        if (data_labels[0]==1):\n",
    "            small_pred_probs = np.array([[0,1] for i in range(len(test_data_labels))])\n",
    "        else:\n",
    "            small_pred_probs = np.array([[1,0] for i in range(len(test_data_labels))])\n",
    "    else: \n",
    "        small_logistic_regression = LogisticRegression(max_iter=1000,solver=\"liblinear\",random_state=19)\n",
    "        small_logistic_regression.fit(data_features, data_labels)\n",
    "        small_pred_probs = small_logistic_regression.predict_proba(test_data_features)\n",
    "\n",
    "    small_loss = 20-1*log_loss(test_data_labels, small_pred_probs)\n",
    "    return small_loss\n",
    "\n",
    "def utilityRF(target_data, test_data, data_owner_data):\n",
    "\n",
    "    data_owner_data = [int(x) for x in data_owner_data]\n",
    "    data = target_data.loc[data_owner_data]\n",
    "    data_features = data.drop(\"target\", axis=1).values\n",
    "    data_labels = data[\"target\"].values\n",
    "    test_data_features = test_data.drop(\"target\", axis=1).values\n",
    "\n",
    "\n",
    "    test_data_labels = test_data[\"target\"].values\n",
    "\n",
    "    \n",
    "    # Create and train the random forest classifier\n",
    "    small_random_forest = RandomForestClassifier()\n",
    "    small_random_forest.fit(data_features, data_labels)\n",
    "    \n",
    "    # Predict probabilities for the test data\n",
    "    small_pred_probs = small_random_forest.predict(test_data_features)\n",
    "    # prediction_classes = small_pred_probs.classes_\n",
    "\n",
    "    # final_pred_probs = np.zeros((len(test_data), len(set(test_data_labels))))\n",
    "    \n",
    "    # Calculate the log loss\n",
    "    small_loss = accuracy_score(small_pred_probs, test_data_labels)\n",
    "    return small_loss\n",
    "\n",
    "def assignClusters(df, cat):\n",
    "    unique_categories = df[cat].unique()\n",
    "\n",
    "    # Create a dictionary to store the lists of indices\n",
    "    owner_data = {}\n",
    "    category_indices = {}\n",
    "\n",
    "    # Iterate through unique values and extract the indices\n",
    "    for i in range(len(unique_categories)):\n",
    "        category = unique_categories[i]\n",
    "        owner_data[i] = df.index[df[cat] == category].tolist()\n",
    "        category_indices[i] = category\n",
    "\n",
    "    print(owner_data)\n",
    "    return owner_data, category_indices\n",
    "\n",
    "def kernel_density_estimation(data, bandwidth=0.2, kernel='gaussian', x_grid=None):\n",
    "    # Fit the CF model\n",
    "    data = np.array(data)\n",
    "    kde = KernelDensity(bandwidth=bandwidth, kernel=kernel)\n",
    "    kde.fit(data[:,np.newaxis])\n",
    "\n",
    "    # If x_grid is not provided, create a default grid of values\n",
    "    if x_grid is None:\n",
    "        x_grid = np.linspace(data.min(), data.max(), 1000)\n",
    "\n",
    "    # Compute the log-density values for the grid of values\n",
    "    log_dens = kde.score_samples(x_grid[:,np.newaxis])\n",
    "\n",
    "    # Exponentiate the log-densities to obtain the density values\n",
    "    density = np.exp(log_dens)\n",
    "\n",
    "    return x_grid, density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "\n",
    "def flatten(lst): return [item for sublist in lst for item in sublist]\n",
    "\n",
    "def powerset(iterable): \n",
    "    s = list(iterable) \n",
    "    return itertools.chain.from_iterable(itertools.combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "def shapley(target_data, test, owners, A):\n",
    "    A_data = owners[A]\n",
    "    # B_data = owners[B]\n",
    "    list_owners = list(owners.keys())\n",
    "    list_owners.remove(A)\n",
    "    # list_owners.remove(B)\n",
    "    coalitions = powerset(list_owners)\n",
    "    total = 0\n",
    "\n",
    "    # go through all the powersets in order of size\n",
    "    for S in coalitions:\n",
    "        P_data = []\n",
    "        for o in S:\n",
    "            P_data.append(owners[o])\n",
    "        P_data = list(flatten(P_data))\n",
    "        owners.update({A: list(np.append(owners[A], P_data))})\n",
    "        # owners.update({B: list(np.append(owners[B], P_data))})\n",
    "        P_util = utilityFunc(target_data, target_data, test, P_data, \"ha\") if len(P_data) != 0 else 0\n",
    "        addend = 1/(math.comb(len(list(owners.keys()))-1,len(S))) * (utilityFunc(target_data, target_data, test, owners[A], \"ha\") - P_util)\n",
    "        # print(\"utility A and P\", utilityFunc(target_data, target_data, test, owners[A], \"ha\"))\n",
    "        # print(\"utility P\", P_util)\n",
    "        total += addend\n",
    "        owners.update({A: A_data})\n",
    "        # owners.update({B: B_data})\n",
    "    return total/len(owners)\n",
    "\n",
    "def diffShapleyCF(all_train_data, target_data, test, owners, A, B, util):\n",
    "    A_data = owners[A]\n",
    "    B_data = owners[B]\n",
    "    list_owners = list(owners.keys())\n",
    "    list_owners.remove(A)\n",
    "    list_owners.remove(B)\n",
    "    coalitions = powerset(list_owners)\n",
    "    total = 0\n",
    "\n",
    "    # go through all the powersets in order of size\n",
    "    for S in coalitions:\n",
    "        P_data = []\n",
    "        for o in S:\n",
    "            P_data.append(owners[o])\n",
    "        P_data = list(flatten(P_data))\n",
    "        owners.update({A: list(np.append(owners[A], P_data))})\n",
    "        owners.update({B: list(np.append(owners[B], P_data))})\n",
    "        total += 1/((len(S)+1)*math.comb(len(list(owners.keys()))-1,len(S)+1)) * diffUtilityCF(all_train_data, target_data, test, owners, A, B, util)\n",
    "        owners.update({A: A_data})\n",
    "        owners.update({B: B_data})\n",
    "    return total\n",
    "\n",
    "def diffUtilityCF(all_train_data, target_data, test_data, owners, o1, o2, util='decision_tree'):\n",
    "    return utilityFunc(all_train_data, target_data, test_data, owners[o1], util) - utilityFunc(all_train_data, target_data, test_data, owners[o2],util)\n",
    "\n",
    "# %%\n",
    "def utilityFunc(all_train_data, target_data, test_data, data_owner_data, util):\n",
    "    return utilityLR(target_data, test_data, data_owner_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_mini_train = pd.read_csv('bc_mini_train.csv')\n",
    "bc_mini_test = pd.read_csv('bc_mini_test.csv')\n",
    "\n",
    "owners = {0: [0, 1, 2, 3], 1: [4, 5, 10, 11], 2: [12, 13, 14, 15, 16, 17, 18], 4:[6, 7, 8, 9]}\n",
    "print(shapley(bc_mini_train, bc_mini_test, owners, 0))\n",
    "print(findShapleyMC(bc_mini_train, bc_mini_test, 0, owners, 10000))\n",
    "print(shapley(bc_mini_train, bc_mini_test, owners, 2) - shapley(bc_mini_train, bc_mini_test, owners, 1))\n",
    "print(diffShapleyCF(bc_mini_train, bc_mini_train, bc_mini_test, owners, 2, 1, utilityLR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reservations_train = pd.read_csv('reservation_train.csv')\n",
    "reservations_test = pd.read_csv('reservation_test.csv')\n",
    "\n",
    "owners, category_indices = assignClusters(reservations_train, 'arrival_month')\n",
    "\n",
    "for i in range(len(owners)):\n",
    "    # print(len(owners[i]))\n",
    "    print(category_indices[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key_by_value(dictionary, value):\n",
    "    for key, val in dictionary.items():\n",
    "        if val == value:\n",
    "            return key\n",
    "    return None\n",
    "for i in range(len(owners)):\n",
    "    month = get_key_by_value(category_indices,i+1)\n",
    "    print(len(owners[month]))\n",
    "    print(utilityLR(reservations_train, reservations_test, owners[month]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(owners)):\n",
    "    month = get_key_by_value(category_indices,i+1)\n",
    "    print(findShapleyMC(reservations_train, reservations_test, month, owners, 5000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
